{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ae8a2b",
   "metadata": {},
   "source": [
    "<picture>\n",
    "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://vespa.ai/assets/vespa-ai-logo-heather.svg\">\n",
    "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://vespa.ai/assets/vespa-ai-logo-rock.svg\">\n",
    "  <img alt=\"#Vespa\" width=\"200\" src=\"https://vespa.ai/assets/vespa-ai-logo-heather.svg\" style=\"margin-bottom: 25px;\">\n",
    "</picture>\n",
    "\n",
    "\n",
    "# BGE-M3 - The Mother of all embedding models\n",
    "\n",
    "BAAI released BGE-M3 on January 30th, a new member of the BGE model series. \n",
    "\n",
    "> M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec (colbert) retrieval).\n",
    "\n",
    "This notebook demonstrates how to use [BGE_M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3) embeddings and \n",
    "represent all three representations in Vespa! The only scalable serving engine that can handle all M3 representations.\n",
    "\n",
    "This code is inspired by the README from the model hub [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3).\n",
    "\n",
    "\n",
    "Let's get started! First, install dependencies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install -U pyvespa FlagEmbedding "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ffa3cbe"
  },
  {
   "cell_type": "markdown",
   "id": "33c8d886",
   "metadata": {},
   "source": [
    "### Explore the multiple representations of M3\n",
    "When encoding text, we can ask for the representations we want\n",
    "\n",
    "- Sparse (SPLADE) vectors \n",
    "- Dense (DPR) regular text embeddings \n",
    "- Multi-Dense (ColBERT) - contextualized multi-token vectors \n",
    "\n",
    "Let us dive into it - To use this model on CPU, we set `use_fp16` to False, for GPU inference, it is recommended to use `use_fp16=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4776f0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T19:19:59.451433499Z",
     "start_time": "2024-02-04T19:19:53.113524513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonkulaga/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6c427666aa24f1da630415aae426226"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing colbert_linear and sparse_linear---------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97c414",
   "metadata": {},
   "source": [
    "## A demo passage \n",
    "\n",
    "Let us encode a simple passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06045105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:26:15.189078390Z",
     "start_time": "2024-02-04T13:26:15.180856198Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "passage = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc307bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:26:16.517325769Z",
     "start_time": "2024-02-04T13:26:15.185714204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding:   0%|          | 0/1 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "encoding: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "passage_embeddings = model.encode(passage, return_dense=True, return_sparse=True, return_colbert_vecs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(30, 1024)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_embeddings['colbert_vecs'][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:36:36.998439066Z",
     "start_time": "2024-02-04T14:36:36.902206273Z"
    }
   },
   "id": "1e739c7bc842a9a"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(1024,)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_embeddings['dense_vecs'][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:37:12.121229257Z",
     "start_time": "2024-02-04T14:37:12.073458445Z"
    }
   },
   "id": "4a25588f89645a0f"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[defaultdict(int,\n             {'335': 0.14094123,\n              '11679': 0.25865352,\n              '276': 0.17205116,\n              '363': 0.2689343,\n              '83': 0.12733835,\n              '142': 0.073539406,\n              '55720': 0.21414939,\n              '59725': 0.16704923,\n              '3299': 0.25499487,\n              '8060': 0.19095251,\n              '214': 0.0827628,\n              '168': 0.18121913,\n              '184': 0.1212738,\n              '456': 0.057080604,\n              '97351': 0.15733702,\n              '1405': 0.06340542,\n              '75675': 0.15143114,\n              '21533': 0.10568179,\n              '14858': 0.15083122,\n              '136': 0.015894324,\n              '6024': 0.08422447,\n              '272': 0.14546244,\n              '18770': 0.14017706,\n              '182809': 0.15259874})]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_embeddings['lexical_weights']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:37:56.704176886Z",
     "start_time": "2024-02-04T14:37:56.683211556Z"
    }
   },
   "id": "38a98891e5267ac5"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_embeddings.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:36:52.382227013Z",
     "start_time": "2024-02-04T14:36:52.349669899Z"
    }
   },
   "id": "8de0d471c5986610"
  },
  {
   "cell_type": "markdown",
   "id": "da356d25",
   "metadata": {},
   "source": [
    "## Defining the Vespa application\n",
    "[PyVespa](https://pyvespa.readthedocs.io/en/latest/) helps us build the [Vespa application package](https://docs.vespa.ai/en/application-packages.html). \n",
    "A Vespa application package consists of configuration files, schemas, models, and code (plugins).   \n",
    "\n",
    "First, we define a [Vespa schema](https://docs.vespa.ai/en/schemas.html) with the fields we want to store and their type. We\n",
    "use Vespa [tensors](https://docs.vespa.ai/en/tensor-user-guide.html) to represent the 3 different M3 representations. \n",
    "\n",
    "- We use a mapped tensor denoted by `t{}` to represent the sparse lexical representation \n",
    "- We use an indexed tensor denoted by `x[1024]` to represent the dense single vector representation of 1024 dimensions\n",
    "- For the colbert_rep (multi vector), we use a mixed tensor that combines a mapped and an indexed dimension. \n",
    "\n",
    "To save resource footprint, we use `bfloat16` tensor cell type, this saves 50% storage compared to `float`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dca2378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:26:16.565502563Z",
     "start_time": "2024-02-04T13:26:16.522126699Z"
    }
   },
   "outputs": [],
   "source": [
    "from vespa.package import Schema, Document, Field, FieldSet\n",
    "m_schema = Schema(\n",
    "            name=\"m3\",\n",
    "            document=Document(\n",
    "                fields=[\n",
    "                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n",
    "                    Field(name=\"text\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"lexical_rep\", type=\"tensor<bfloat16>(t{})\", indexing=[\"summary\", \"attribute\"]),\n",
    "                    Field(name=\"dense_rep\", type=\"tensor<bfloat16>(x[1024])\", indexing=[\"summary\", \"attribute\"], attribute=[\"distance-metric: angular\"]),\n",
    "                    Field(name=\"colbert_rep\", type=\"tensor<bfloat16>(t{}, x[1024])\", indexing=[\"summary\", \"attribute\"])\n",
    "                ],\n",
    "            ),\n",
    "            fieldsets=[\n",
    "                FieldSet(name = \"default\", fields = [\"text\"])\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834fe25",
   "metadata": {},
   "source": [
    "The above defines our `m` schema with the original text and the three different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c5da1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:26:16.571158652Z",
     "start_time": "2024-02-04T13:26:16.565783411Z"
    }
   },
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "vespa_app_name = \"m3\"\n",
    "vespa_application_package = ApplicationPackage(\n",
    "        name=vespa_app_name,\n",
    "        schema=[m_schema]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3d7bd",
   "metadata": {},
   "source": [
    "In the last step, we configure [ranking](https://docs.vespa.ai/en/ranking.html) by adding `rank-profile`'s to the schema. \n",
    "\n",
    "\n",
    "We define three functions that implement the three different scoring functions for the different representations\n",
    "\n",
    "- dense (dense cosine similarity)\n",
    "- sparse (sparse dot product)\n",
    "- max_sim (The colbert max sim operation)\n",
    "\n",
    "Then, we combine these three scoring functions using a linear combination with weights, as suggested\n",
    "by the authors [here](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#compute-score-for-text-pairs). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ce5624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:26:16.585989174Z",
     "start_time": "2024-02-04T13:26:16.572370559Z"
    }
   },
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function,  FirstPhaseRanking\n",
    "\n",
    "\n",
    "semantic = RankProfile(\n",
    "    name=\"m3hybrid\", \n",
    "    inputs=[\n",
    "        (\"query(q_dense)\", \"tensor<bfloat16>(x[1024])\"), \n",
    "        (\"query(q_lexical)\", \"tensor<bfloat16>(t{})\"), \n",
    "        (\"query(q_colbert)\", \"tensor<bfloat16>(qt{}, x[1024])\"),\n",
    "        (\"query(q_len_colbert)\", \"float\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(\n",
    "            name=\"dense\",\n",
    "            expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"lexical\",\n",
    "            expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"max_sim\",\n",
    "            expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\"\n",
    "        )\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\",\n",
    "        rank_score_drop_limit=0.0\n",
    "    ),\n",
    "    match_features=[\"dense\", \"lexical\", \"max_sim\"]\n",
    ")\n",
    "m_schema.add_rank_profile(semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78268c",
   "metadata": {},
   "source": [
    "The `m3hybrid` rank-profile above defines the query input embedding type and a similarities function that\n",
    "uses a Vespa [tensor compute function](https://docs.vespa.ai/en/reference/ranking-expressions.html#tensor-functions) that calculates\n",
    "the M3 similarities for dense, lexical, and the max_sim for the colbert representations. \n",
    "\n",
    "The profile only defines a single ranking phase, using a linear combination of multiple features using the suggested weighting.\n",
    "\n",
    "Using [match-features](https://docs.vespa.ai/en/reference/schema-reference.html#match-features), Vespa\n",
    "returns selected features along with the hit in the SERP (result page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server, 0/300 seconds...\n",
      "Waiting for configuration server, 5/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 0/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 5/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 10/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Application is up!\n",
      "Finished deployment.\n"
     ]
    }
   ],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "app = vespa_docker.deploy(application_package=vespa_application_package, debug=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T13:27:15.810693525Z",
     "start_time": "2024-02-04T13:26:44.642680712Z"
    }
   },
   "id": "a93e95b5d5916c9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now deploy the app to Vespa Cloud dev zone. \n",
    "\n",
    "The first deployment typically takes 2 minutes until the endpoint is up. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9baa5a"
  },
  {
   "cell_type": "markdown",
   "id": "b7d54bd1",
   "metadata": {},
   "source": [
    "# Feed the M3 representations\n",
    "\n",
    "We convert the three different representations to Vespa feed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1674c66e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:27:15.823469247Z",
     "start_time": "2024-02-04T13:27:15.809166410Z"
    }
   },
   "outputs": [],
   "source": [
    "vespa_fields = {\n",
    "    \"text\": passage[0],\n",
    "    \"lexical_rep\": {key: float(value) for key, value in passage_embeddings['lexical_weights'][0].items()},\n",
    "    \"dense_rep\":passage_embeddings['dense_vecs'][0].tolist(),\n",
    "    \"colbert_rep\":  {index: passage_embeddings['colbert_vecs'][0][index].tolist() for index in range(passage_embeddings['colbert_vecs'][0].shape[0])}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d36138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:27:16.431930709Z",
     "start_time": "2024-02-04T13:27:15.817610143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<vespa.io.VespaResponse at 0x706034a586a0>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.feed_data_point(schema='m3', data_id=0, fields=vespa_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'http://localhost'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T13:35:55.905100978Z",
     "start_time": "2024-02-04T13:35:55.874522136Z"
    }
   },
   "id": "f1d41ecd0d2d0381"
  },
  {
   "cell_type": "markdown",
   "id": "20b007ec",
   "metadata": {},
   "source": [
    "### Querying data\n",
    "\n",
    "Now, we can also query our data. \n",
    "\n",
    "Read more about querying Vespa in:\n",
    "\n",
    "- [Vespa Query API](https://docs.vespa.ai/en/query-api.html)\n",
    "- [Vespa Query API reference](https://docs.vespa.ai/en/reference/query-api-reference.html)\n",
    "- [Vespa Query Language API (YQL)](https://docs.vespa.ai/en/query-language.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810b99d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:27:24.678967758Z",
     "start_time": "2024-02-04T13:27:24.026658367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding:   0%|          | 0/1 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "encoding: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "query  = [\"What is BGE M3?\"]\n",
    "query_embeddings = model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533ecc3",
   "metadata": {},
   "source": [
    "The M3 colbert scoring function needs the query length to normalize the score to the range 0 to 1. This helps when combining\n",
    "the score with the other scoring functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1c75ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:27:44.765906926Z",
     "start_time": "2024-02-04T13:27:44.718250644Z"
    }
   },
   "outputs": [],
   "source": [
    "query_length = query_embeddings['colbert_vecs'][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc09c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:27:56.276205312Z",
     "start_time": "2024-02-04T13:27:56.261775632Z"
    }
   },
   "outputs": [],
   "source": [
    "query_fields = {\n",
    "    \"input.query(q_lexical)\": {key: float(value) for key, value in query_embeddings['lexical_weights'][0].items()},\n",
    "    \"input.query(q_dense)\": query_embeddings['dense_vecs'][0].tolist(),\n",
    "    \"input.query(q_colbert)\":  str({index: query_embeddings['colbert_vecs'][0][index].tolist() for index in range(query_embeddings['colbert_vecs'][0].shape[0])}),\n",
    "    \"input.query(q_len_colbert)\": query_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9349fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:28:10.200259300Z",
     "start_time": "2024-02-04T13:28:10.061071591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"index:m3_content/0/cfcd208456509d9b37146efc\",\n",
      "  \"relevance\": 0.5993382421532011,\n",
      "  \"source\": \"m3_content\",\n",
      "  \"fields\": {\n",
      "    \"matchfeatures\": {\n",
      "      \"dense\": 0.6259023168183205,\n",
      "      \"lexical\": 0.1941967010498047,\n",
      "      \"max_sim\": 0.7753449380397797\n",
      "    },\n",
      "    \"text\": \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from vespa.io import VespaQueryResponse\n",
    "import json\n",
    "\n",
    "response:VespaQueryResponse = app.query(\n",
    "    yql=\"select id, text from m3 where ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",\n",
    "    ranking=\"m3hybrid\",\n",
    "    body={\n",
    "        **query_fields\n",
    "    }\n",
    ")\n",
    "assert(response.is_successful())\n",
    "print(json.dumps(response.hits[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ca1da",
   "metadata": {},
   "source": [
    "Notice the `matchfeatures` that returns the configured match-features from the rank-profile. We can \n",
    "use these to compare the torch model scoring with the computations specified in Vespa. \n",
    "\n",
    "Now, we can compare the Vespa computed scores with the model torch code and they line up perfectly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06f7b1c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:18:59.717872Z",
     "start_time": "2024-02-04T13:18:59.666887187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.1955444384366274"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_lexical_matching_score(passage_embeddings['lexical_weights'][0], query_embeddings['lexical_weights'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9af5c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6259037"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings['dense_vecs'][0] @ passage_embeddings['dense_vecs'][0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bcf591c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:19:04.754646434Z",
     "start_time": "2024-02-04T13:19:04.708132295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.7797)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.colbert_score(query_embeddings['colbert_vecs'][0],passage_embeddings['colbert_vecs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dd861",
   "metadata": {},
   "source": [
    "### That is it! \n",
    "\n",
    "That is how easy it is to represent the brand new M3 FlagEmbedding representations in Vespa! Read more in the \n",
    "[M3 technical report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf). \n",
    "\n",
    "We can go ahead and delete the Vespa cloud instance we deployed by:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
