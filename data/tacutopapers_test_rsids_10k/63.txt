Another important facet that has to be carefully pondered is the high phyla bias of the structural data when compared to the baseline phyla distribution of the UniRef50 database. As can be seen from Figure 1c, around 50% of the repeats in ID50 are of mammalian origin while the UniRef50 baseline is of less than 3% in both annotated LRR proteins or any protein. Moreover, the ≈20% plant LRR motifs present in ID50 originate overwhelmingly from RLP and RLK proteins while plant NLRs are poorly represented in this set, with only a single 3D structure recently reported for the ZAR1 NLR protein from Arabidopsis thaliana [21,22]. (PDB: 6J5W). The hydrophobic positions in the minimal 'L 0 xxL 3 xL 5 ' motif are shown in orange. The first N-entry repeat (blue) and the last C-exit repeat (red) are also mapped on the structure. (c) Phyla distribution of the initial LRR motif set ID90, the 50% identity trimmed LRR motifs set (ID50), annotated LRR proteins and all proteins from the UniRef50 database (from left to right). Percent values corresponding to the mammals group are shown in red. (d) Frequency plot of amino acid composition of the N-entry, core and C-exit motifs on the 50% identity trimmed set. Amino acids are colored according to their properties as follows: hydrophobic (yellow), acidic (red), basic (blue), asparagine and glutamine (purple), proline and glycine (green), others (black).

(e) Jensen-Shannon divergence (JSD) score for each position of the LRR motif at different identity thresholds. Higher values show increased conservation.

Development of the LRRpredictor Method

In order to train a machine learning (ML) estimator for detecting LRR motifs we used an overall dataset comprising the filtered LRR ID50 dataset and a collection of 875 non-LRR domains composed of one representative of each CATH topology ( Figure 2a).

As discussed in Section 1, the sequence patterns corresponding to the ≈850 actual true structural LRR motifs identified in ID50 are quite common in any protein. We will name here such sequence patterns as potential motifs. As expected, Table 1 shows that potential motifs occur with more or less equal probability in both the LRR and non-LRR domains of the overall dataset. Moreover, even when taking into account only LRR domains the number of potential motifs is larger than the number of true structural LRR motifs (Table 1). This allows the ML estimators to learn to detect true motifs from the far larger set of potential motifs by taking into account the larger 16 amino acid sequence context in which the true motifs are embedded. In this way the method developed herein can be used not only to delineate repeats in a given LRR domain but also to discriminate between protein products that do not have LRR domains from those hosting such domains. In developing LRRpredictor we tested 'sequence-based' features based on position-specific scoring matrices-PSSMs either solely or combined with 'structural-based' features as described in Section 2 ( Figure 2c). PSSM profiles are expected to provide context information on the overall sequence, to highlight the key amino acids position that are conserved, as the amino acids scores are derived from amino acid substitution probabilities conditioned by the homologues family they belong to.

Therefore, it is expected that irregular LRR motifs would be more detectable when using sequence profiles, rather than amino acid sequence alone. scoring matrices-PSSMs either solely or combined with 'structural-based' features as described in Section 2 ( Figure 2c). PSSM profiles are expected to provide context information on the overall sequence, to highlight the key amino acids position that are conserved, as the amino acids scores are derived from amino acid substitution probabilities conditioned by the homologues family they belong to. Therefore, it is expected that irregular LRR motifs would be more detectable when using sequence profiles, rather than amino acid sequence alone. The dataset was split into five parts: one part was initially separated as test set and the other four were used as a training set in parameter tuning using a four-fold cross-validation (CV) approach, where models were iteratively trained on three of the CV sets and tested on the remaining fourth ( Figure 2b). A pool of estimators (representing algorithms for classification) that used either (1) sequence-based or (2) both sequence and structural features and (3) various imbalance class treatments were optimized via cross-validation. Finally, best performing estimators were studied in the context of an ensemble estimator.

The selected ensemble classifier, further referred to as LRRpredictor is a soft voter aggregating eight classifiers C1-C8 (Figure 2d) which were trained to detect the LRR motif starting position-i.e., L 0 position from the minimalistic LRR motif 'L 0 xxL 3 xL 5 '.

Finally, LRRpredictor was trained on the entire training set (all four CV sets) and tested on the test set which had been set aside.

Assessment of LRRpredictor Performance

The precision of LRRpredictor given by the fraction of true-positives (TP) predicted results over the sum of true-positives (TP) and false-positives (FP) varies between 89% and 97% on the test set and within cross-validation sets (Figure 3a). Similarly, the recall (also known as sensitivity), given by the fraction of TP over TP + false-negatives (FN) varies between 85% and 93%, while the F1-score (representing the harmonic mean between precision and recall) varies between 87% and 95% on the test set and cross-validation sets (Figure 3a,b). [26] and LRRsearch [27] (computed on their webservers using default parameters).

As seen in Figure 4a, repeat lengths are rarely found outside the 19-35aa range, cases in which prediction becomes ambiguous. Too short repeats are improbable due to structural constraints and might indicate false positive predictions. Similarly, too long repeats-over 40 amino acids-could indicate either the presence of undetected repeats (false negatives) or cases in which an insertion or 'island' shapes up protruding the horseshoe structure (Figure 4a). Very large gaps between LRR motifs (more than 100 aa) were not included in computing the length distribution as these are rather indicating the presence of an inserted domain flanked by two LRR domains.

We further analyzed the percent of the annotated LRR domain span that is covered by LRRpredictor and compared the predicted LRR motifs to LRRfinder [26] and LRRsearch [27] predictions and to the existing motif annotations from Interpro collection. In doing so we defined as predicted repeats motifs separated by 15-35 amino acids. Predicted motifs that superpose or cluster within 15 amino acids were counted only once, while when the distance between two motifs was higher than 35, the repeat was considered to be a potential terminal repeat or contain a domain break and the first 24 aa of such a stretch was assigned as a predicted repeat, given that this is the most frequent repeat length over the structural data. [26] and LRRsearch [27] (computed on their webservers using default parameters).

LRRpredictor Behavior on Protein Families Containing LRR Domains

As the available structural data is scarce, we further evaluated the extrapolation capabilities of LRRpredictor on a set of LRR domains annotated in Interpro collection. Groups of the most representative protein functional classes containing LRR domains were generated as follows: four groups from flowering plants-resistance proteins (CNL plants and TNL plants ) and extracellular receptors (RLK plants and RLP plants ) and two groups from vertebrates-NLR vert and TLR vert as described in Section 2.

Selected sequences from each group were subjected to LRRpredictor motif detection. The repeat length distribution of the predicted LRR repeats (Figure 4a), is consistent with previously reported lengths within all protein groups of the seven type Kobe-Kajava (KK) classification [14,65]. The repeat length distribution of extracellular LRR domains (RLK plants , RLP plants , and TLR vert ) show a sharp peak at 24 amino acids, in agreement with the most frequent repeat length within plant-specific (PS) from KK classification [14,65]. As they often contain large helices over the dorsal side of the LRR horseshoe, vertebrate NLRs repeats have longer lengths (25-30 aa) as previously shown by the same classification, while plant NLRs (CNL plants and TNL plants ) have a larger distribution with a lower peak shaping up toward lower value side (20-24 aa) of repeat lengths range. (Figure 4b). For all six receptor classes analyzed herein, both LRRfinder and LRRsearch slightly increase the LRR coverage as compared to Interpro annotations especially in the case of extracellular receptors, with LRRsearch surpassing LRRfinder in the case of plant NLRs (Figure 4b).

As also can be seen from Figure 4b, in comparison to Interpro and the two predictors mentioned above, LRRpredictor covers far larger regions of LRR domains with coverage percentages (CP) exceeding 60% and almost complete coverage in over 50% in all six groups (Figure 4b). It is interesting to note that Interpro annotation of extracellular LRR domains also include the N-terminal cap region, that is not formally a LRR repeat. This results in the fact that LRRpredictor covers in most cases only ≈90% of this domain, instead of 100% as in NLR groups (Figure 4b).

Predicted Repeats Consensus in Each Class